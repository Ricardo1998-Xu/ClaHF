# ClaHF: A Human Feedback-inspired Reinforcement Learning Framework for Improving Classification Tasks

## ğŸš€ Overview
**ClaHF**: a human feedback-inspired reinforcement learning framework for text classification that integrates preference modeling and RL optimization into the classification pipeline without requiring additional human annotations. The overall framework of ClaHF is illustrated in Figure. (a) SFT to provide high-quality initialization. (b) Automatic construction of preference data from the original classification dataset. (c) Training the RM with preference data. (d) RL optimization of the policy model using the trained RM.

![image](figure/fig2.png?raw=true)

This repository provides an end-to-end implementation of ClaHF, including:
- Supervised Fine-Tuning (SFT)
- Reward Model training with pairwise preferences
- PPO-based optimization for classification models
- Adaptive KL control and evaluation on multiple datasets
  
---

## ğŸ“‚ Repository Structure
```bash
ClaHF/
â”‚â”€â”€ ğŸ“ Dataset/                     # Contains datasets used in the study
â”‚   â”œâ”€â”€ ğŸ“‚ CoLA/                    # The CoLA dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ train.jsonl          # Training dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ test.jsonl           # Testing dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ valid.jsonl          # valid dataset
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ ğŸ“‚ MRPC/
â”‚   â”œâ”€â”€ ğŸ“‚ SST-5/              
â”‚   â””â”€â”€ ...               
â”‚
â”‚â”€â”€ ğŸ“ Pre_Dataset/                 # Preference datasets
â”‚   â”œâ”€â”€ ğŸ“‚ CoLA/                    # The CoLA dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ train.jsonl          # Training dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ test.jsonl           # Testing dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ valid.jsonl          # valid dataset
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ ğŸ“‚ MRPC/
â”‚   â”œâ”€â”€ ğŸ“‚ SST-5/              
â”‚   â””â”€â”€ ...                    
â”‚
â”‚â”€â”€ ğŸ“ Code/            # Implementations of classification models
â”‚   â”œâ”€â”€ ğŸ¤– Test1_bert/              # BERT model implementation
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ clss_indices.json    # Label mapping file
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ model.py             # Model definition
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ RewardModel.py       # Reward model definition
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ run.py               # Script for fine-tuning the model
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ run_RL.py            # RL optimization
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ run_RM.py            # Script for training the RM
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ test.py              # Script for model evaluation
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ ğŸ¤– Test2_codebert/          # CodeBERT model implementation
â”‚   â”œâ”€â”€ ğŸ¤– Test3_t5/                # T5 model implementation
â”‚   â”œâ”€â”€ ğŸ¤– Test4_codet5/            # CodeT5 model implementation
â”‚   â”œâ”€â”€ ğŸ¤– Test5_codet5+/           # CodeT5+ model implementation
â”‚   â”œâ”€â”€ ğŸ¤– Test6_opt/               # OPT model implementation
â”‚   â”œâ”€â”€ ğŸ¤– Test7_codegen/           # CodeGen model implementation
â”‚   â””â”€â”€ ğŸ¤– Test8_qwen3/             # QWen3 model implementation
â”‚   
â”‚â”€â”€ ğŸ“œ environment.yaml             # Environment configuration file
â”‚â”€â”€ ğŸ“œ README.md                    
â””â”€â”€ ...
```

---

## ğŸ’» Experiments
### ğŸ“¥ Install
```sh
conda env create -f environment.yml
```

### ğŸš€ Training Pipeline

#### Step 1: Supervised Fine-Tuning (SFT)
Train a base classifier with labeled data.
```sh
python run.py \
    --num_labels=5 \
    --train_data_file=. \
    --eval_data_file=. \
    --output_dir=./saved_models \
    --runs_path=./runs \
    --model_type=qwen3 \
    --tokenizer_name=Qwen/Qwen3-0.6B \
    --model_name_or_path=Qwen/Qwen3-0.6B \
    --do_train \
    --epoch 10 \
    --block_size 400 \
    --train_batch_size 16 \
    --eval_batch_size 16 \
    --learning_rate 2e-5 \
    --max_grad_norm 1.0 \
    --gradient_accumulation_steps 1
    --adam_epsilon 1e-8
    --evaluate_during_training \
    --seed 123456
```

#### Step 2: Reward Model Training
Then train the reward model with Top-1 + Pairwise Loss:
```sh
python run_RM.py \
    --train_data_file=. \
    --eval_data_file=. \
    --output_dir=./saved_models \
    --runs_path=./runs \
    --model_type=qwen3 \
    --tokenizer_name=Qwen/Qwen3-0.6B \
    --model_name_or_path=Qwen/Qwen3-0.6B \
    --do_train \
    --epoch 10 \
    --block_size 400 \
    --train_batch_size 16 \
    --eval_batch_size 16 \
    --learning_rate 1e-5 \
    --max_grad_norm 1.0 \
    --gradient_accumulation_steps 1
    --adam_epsilon 1e-8
    --evaluate_during_training \
    --seed 123456
```

#### Step 3: PPO Optimization
Use the SFT model as policy initialization and optimize with reward feedback.
```sh
python run_RL.py \
    --num_labels=5 \
    --json_path=./SST-5.json \
    --train_data_file=. \
    --eval_data_file=. \
    --sft_path=checkpoints/sft \
    --reward_path=checkpoints/reward \
    --output_dir=./saved_models \
    --runs_path=./runs \
    --model_type=qwen3 \
    --tokenizer_name=Qwen/Qwen3-0.6B \
    --model_name_or_path=Qwen/Qwen3-0.6B \
    --do_train \
    --epoch 10 \
    --clip_range 0.2 \
    --vf_coef 0.25
    --block_size 400 \
    --train_batch_size 16 \
    --eval_batch_size 16 \
    --learning_rate 1e-6 \
    --max_grad_norm 1.0 \
    --gradient_accumulation_steps 1
    --adam_epsilon 1e-8
    --evaluate_during_training \
    --seed 123456
```

#### Example: Evaluation
```sh
python test.py \
    --test_data_file=. \
    --output_dir=./saved_models \
    --results_path=./results \
    --model_type=qwen3 \
    --tokenizer_name=Qwen/Qwen3-0.6B \
    --model_name_or_path=Qwen/Qwen3-0.6B \
    --do_test \
    --block_size 400 \
    --eval_batch_size 8 \
    --seed 123456
```
Metrics include:
Accuracy, F1, Expected Calibration Error (ECE), MCC

---

## ğŸ“œ License
This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---
